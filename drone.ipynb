{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNWTdMC9zqnKdbvDdw5e9sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grzegorzj/drone-detection/blob/main/drone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "othPpj11Uho3",
        "outputId": "dc21b60f-e122-459f-b7f5-345b6364ead9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DroneAudioDataset'...\n",
            "remote: Enumerating objects: 10649, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 10649 (delta 5), reused 5 (delta 5), pack-reused 10643 (from 1)\u001b[K\n",
            "Receiving objects: 100% (10649/10649), 274.31 MiB | 15.28 MiB/s, done.\n",
            "Resolving deltas: 100% (181/181), done.\n",
            "Updating files: 100% (23409/23409), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/saraalemadi/DroneAudioDataset.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attempt number one\n",
        "Unconstrained, simple classifier. At the moment, not paying too much attention to size of the model or inference time."
      ],
      "metadata": {
        "id": "4u7rRm27VOHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchaudio numpy scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZLtZlWEVUWO",
        "outputId": "f6fddfe0-c06b-4087-ee10-92356c6bfda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Constants\n",
        "SAMPLE_RATE = 44100  # Standard audio sample rate\n",
        "MAX_DURATION = 5  # Maximum duration in seconds to consider\n",
        "N_MELS = 64  # Number of mel bands\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_mels=N_MELS,\n",
        "            normalized=True\n",
        "        )\n",
        "\n",
        "        # Get all file paths and labels\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Positive examples (yes_drone)\n",
        "        pos_dir = self.root_dir / 'yes_drone'\n",
        "        for file in pos_dir.glob('*.wav'):\n",
        "            self.files.append(file)\n",
        "            self.labels.append(1)\n",
        "\n",
        "        # Negative examples (unknown)\n",
        "        neg_dir = self.root_dir / 'unknown'\n",
        "        for file in neg_dir.glob('*.wav'):\n",
        "            self.files.append(file)\n",
        "            self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load audio\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Convert to mono if stereo\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample if necessary\n",
        "        if sample_rate != SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Pad or trim to MAX_DURATION\n",
        "        target_length = MAX_DURATION * SAMPLE_RATE\n",
        "        if waveform.shape[1] < target_length:\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, target_length - waveform.shape[1]))\n",
        "        else:\n",
        "            waveform = waveform[:, :target_length]\n",
        "\n",
        "        # Convert to mel spectrogram\n",
        "        mel_spec = self.mel_spectrogram(waveform)\n",
        "\n",
        "        # Log scale mel spectrogram\n",
        "        mel_spec = torch.log(mel_spec + 1e-9)\n",
        "\n",
        "        if self.transform:\n",
        "            mel_spec = self.transform(mel_spec)\n",
        "\n",
        "        return mel_spec, label\n",
        "\n",
        "class DroneClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DroneClassifier, self).__init__()\n",
        "\n",
        "        # Lightweight CNN architecture\n",
        "        self.features = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Dropout for regularization\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Adaptive pooling to handle variable input sizes\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print('-' * 60)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_drone_classifier.pth')\n",
        "\n"
      ],
      "metadata": {
        "id": "EMkicXv_VamA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = AudioDataset('DroneAudioDataset/Binary_Drone_Audio')\n",
        "\n",
        "    # Split dataset\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        range(len(dataset)),\n",
        "        test_size=0.2,\n",
        "        stratify=dataset.labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Calculate class weights for imbalanced dataset\n",
        "    labels = np.array(dataset.labels)\n",
        "    class_weights = torch.tensor([\n",
        "        1.0 / (np.sum(labels == 0) / len(labels)),\n",
        "        1.0 / (np.sum(labels == 1) / len(labels))\n",
        "    ], device=device)\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(\n",
        "        torch.utils.data.Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        torch.utils.data.Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    model = DroneClassifier().to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device)"
      ],
      "metadata": {
        "id": "W6cIq15tVl2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iptESFyXV_tA",
        "outputId": "455cd76d-240d-486d-bbcd-889c9763e7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30]\n",
            "Train Loss: 0.7009, Train Acc: 84.59%\n",
            "Val Loss: 0.4721, Val Acc: 96.33%\n",
            "------------------------------------------------------------\n",
            "Epoch [2/30]\n",
            "Train Loss: 0.3569, Train Acc: 94.48%\n",
            "Val Loss: 0.5458, Val Acc: 96.75%\n",
            "------------------------------------------------------------\n",
            "Epoch [3/30]\n",
            "Train Loss: 0.2606, Train Acc: 96.49%\n",
            "Val Loss: 0.1120, Val Acc: 98.80%\n",
            "------------------------------------------------------------\n",
            "Epoch [4/30]\n",
            "Train Loss: 0.1686, Train Acc: 98.03%\n",
            "Val Loss: 0.4785, Val Acc: 77.53%\n",
            "------------------------------------------------------------\n",
            "Epoch [5/30]\n",
            "Train Loss: 0.1443, Train Acc: 98.62%\n",
            "Val Loss: 0.1246, Val Acc: 97.35%\n",
            "------------------------------------------------------------\n",
            "Epoch [6/30]\n",
            "Train Loss: 0.1049, Train Acc: 98.90%\n",
            "Val Loss: 0.0356, Val Acc: 99.27%\n",
            "------------------------------------------------------------\n",
            "Epoch [7/30]\n",
            "Train Loss: 0.0787, Train Acc: 99.21%\n",
            "Val Loss: 0.2158, Val Acc: 96.24%\n",
            "------------------------------------------------------------\n",
            "Epoch [8/30]\n",
            "Train Loss: 0.0786, Train Acc: 99.31%\n",
            "Val Loss: 0.0983, Val Acc: 99.36%\n",
            "------------------------------------------------------------\n",
            "Epoch [9/30]\n",
            "Train Loss: 0.0654, Train Acc: 99.47%\n",
            "Val Loss: 0.9108, Val Acc: 96.28%\n",
            "------------------------------------------------------------\n",
            "Epoch [10/30]\n",
            "Train Loss: 0.0528, Train Acc: 99.39%\n",
            "Val Loss: 0.0125, Val Acc: 99.91%\n",
            "------------------------------------------------------------\n",
            "Epoch [11/30]\n",
            "Train Loss: 0.0363, Train Acc: 99.67%\n",
            "Val Loss: 0.0062, Val Acc: 99.91%\n",
            "------------------------------------------------------------\n",
            "Epoch [12/30]\n",
            "Train Loss: 0.0324, Train Acc: 99.67%\n",
            "Val Loss: 0.2262, Val Acc: 87.83%\n",
            "------------------------------------------------------------\n",
            "Epoch [13/30]\n",
            "Train Loss: 0.0625, Train Acc: 99.40%\n",
            "Val Loss: 0.0209, Val Acc: 99.87%\n",
            "------------------------------------------------------------\n",
            "Epoch [14/30]\n",
            "Train Loss: 0.0426, Train Acc: 99.53%\n",
            "Val Loss: 0.1019, Val Acc: 97.39%\n",
            "------------------------------------------------------------\n",
            "Epoch [15/30]\n",
            "Train Loss: 0.0312, Train Acc: 99.66%\n",
            "Val Loss: 0.0151, Val Acc: 99.70%\n",
            "------------------------------------------------------------\n",
            "Epoch [16/30]\n",
            "Train Loss: 0.0407, Train Acc: 99.55%\n",
            "Val Loss: 0.0193, Val Acc: 99.70%\n",
            "------------------------------------------------------------\n",
            "Epoch [17/30]\n",
            "Train Loss: 0.0399, Train Acc: 99.47%\n",
            "Val Loss: 0.0077, Val Acc: 99.87%\n",
            "------------------------------------------------------------\n",
            "Epoch [18/30]\n",
            "Train Loss: 0.0435, Train Acc: 99.51%\n",
            "Val Loss: 0.0613, Val Acc: 98.93%\n",
            "------------------------------------------------------------\n",
            "Epoch [19/30]\n",
            "Train Loss: 0.0367, Train Acc: 99.55%\n",
            "Val Loss: 0.6849, Val Acc: 95.77%\n",
            "------------------------------------------------------------\n",
            "Epoch [20/30]\n",
            "Train Loss: 0.0235, Train Acc: 99.73%\n",
            "Val Loss: 0.9208, Val Acc: 80.52%\n",
            "------------------------------------------------------------\n",
            "Epoch [21/30]\n",
            "Train Loss: 0.0224, Train Acc: 99.68%\n",
            "Val Loss: 0.0031, Val Acc: 99.96%\n",
            "------------------------------------------------------------\n",
            "Epoch [22/30]\n",
            "Train Loss: 0.0147, Train Acc: 99.79%\n",
            "Val Loss: 0.0184, Val Acc: 99.83%\n",
            "------------------------------------------------------------\n",
            "Epoch [23/30]\n",
            "Train Loss: 0.0453, Train Acc: 99.33%\n",
            "Val Loss: 0.0108, Val Acc: 99.87%\n",
            "------------------------------------------------------------\n",
            "Epoch [24/30]\n",
            "Train Loss: 0.0277, Train Acc: 99.65%\n",
            "Val Loss: 0.0129, Val Acc: 99.91%\n",
            "------------------------------------------------------------\n",
            "Epoch [25/30]\n",
            "Train Loss: 0.0138, Train Acc: 99.78%\n",
            "Val Loss: 0.0083, Val Acc: 99.87%\n",
            "------------------------------------------------------------\n",
            "Epoch [26/30]\n",
            "Train Loss: 0.0286, Train Acc: 99.51%\n",
            "Val Loss: 7.2941, Val Acc: 88.64%\n",
            "------------------------------------------------------------\n",
            "Epoch [27/30]\n",
            "Train Loss: 0.0440, Train Acc: 99.48%\n",
            "Val Loss: 0.3837, Val Acc: 84.84%\n",
            "------------------------------------------------------------\n",
            "Epoch [28/30]\n",
            "Train Loss: 0.0203, Train Acc: 99.80%\n",
            "Val Loss: 0.0180, Val Acc: 99.66%\n",
            "------------------------------------------------------------\n",
            "Epoch [29/30]\n",
            "Train Loss: 0.0220, Train Acc: 99.67%\n",
            "Val Loss: 6.8435, Val Acc: 89.53%\n",
            "------------------------------------------------------------\n",
            "Epoch [30/30]\n",
            "Train Loss: 0.0220, Train Acc: 99.65%\n",
            "Val Loss: 0.0048, Val Acc: 99.96%\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_params(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Total trainable parameters: {total_params:,}')\n",
        "\n",
        "    # Print size of each layer\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(f'{name}: {param.numel():,} parameters')\n",
        "            print(f'Shape: {list(param.shape)}')"
      ],
      "metadata": {
        "id": "Aw-NSeSRdr6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DroneClassifier()\n",
        "print_model_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYs6YdiAdzA1",
        "outputId": "b81ea443-da94-4614-ef27-e470ba3517c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 89,185\n",
            "features.0.weight: 144 parameters\n",
            "Shape: [16, 1, 3, 3]\n",
            "features.0.bias: 16 parameters\n",
            "Shape: [16]\n",
            "features.1.weight: 16 parameters\n",
            "Shape: [16]\n",
            "features.1.bias: 16 parameters\n",
            "Shape: [16]\n",
            "features.4.weight: 4,608 parameters\n",
            "Shape: [32, 16, 3, 3]\n",
            "features.4.bias: 32 parameters\n",
            "Shape: [32]\n",
            "features.5.weight: 32 parameters\n",
            "Shape: [32]\n",
            "features.5.bias: 32 parameters\n",
            "Shape: [32]\n",
            "features.8.weight: 18,432 parameters\n",
            "Shape: [64, 32, 3, 3]\n",
            "features.8.bias: 64 parameters\n",
            "Shape: [64]\n",
            "features.9.weight: 64 parameters\n",
            "Shape: [64]\n",
            "features.9.bias: 64 parameters\n",
            "Shape: [64]\n",
            "classifier.0.weight: 65,536 parameters\n",
            "Shape: [64, 1024]\n",
            "classifier.0.bias: 64 parameters\n",
            "Shape: [64]\n",
            "classifier.3.weight: 64 parameters\n",
            "Shape: [1, 64]\n",
            "classifier.3.bias: 1 parameters\n",
            "Shape: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection under the noise level\n",
        "The first attempt used a straightforward dataset with relatively easy to distinguish samples. Real-life scenario may be harder than that, especially given refractions, noise from other military vehicles, etc. Therefore, we augument the sounds with noise."
      ],
      "metadata": {
        "id": "4RGbCYJuWy8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_esc50(target_dir='/content/datasets'):\n",
        "    \"\"\"\n",
        "    Downloads and prepares the ESC-50 dataset\n",
        "    Returns the path to the noise samples directory\n",
        "    \"\"\"\n",
        "    target_dir = Path(target_dir)\n",
        "    target_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    url = \"https://github.com/karoldvl/ESC-50/archive/master.zip\"\n",
        "    zip_path = target_dir / \"ESC-50-master.zip\"\n",
        "\n",
        "    if not zip_path.exists():\n",
        "        print(\"Downloading ESC-50 dataset...\")\n",
        "        with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                               miniters=1, desc=\"ESC-50\") as t:\n",
        "            urllib.request.urlretrieve(url, filename=zip_path,\n",
        "                                     reporthook=t.update_to)\n",
        "\n",
        "    extract_dir = target_dir / \"ESC-50-master\"\n",
        "    if not extract_dir.exists():\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(target_dir)\n",
        "\n",
        "    noise_dir = target_dir / \"noise_samples\"\n",
        "    noise_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    meta_file = extract_dir / \"meta\" / \"esc50.csv\"\n",
        "    df = pd.read_csv(meta_file)\n",
        "\n",
        "    noise_categories = [\n",
        "        'rain', 'sea_waves', 'crackling_fire', 'crickets',  # nature\n",
        "        'engine', 'train', 'airplane',  # transportation\n",
        "        'wind', 'thunderstorm',  # weather\n",
        "        'crowd', 'footsteps',  # human\n",
        "        'helicopter', 'chainsaw', 'siren'  # mechanical/urban\n",
        "    ]\n",
        "\n",
        "    print(\"Preparing noise samples...\")\n",
        "    audio_dir = extract_dir / \"audio\"\n",
        "\n",
        "    for category in noise_categories:\n",
        "        category_files = df[df['category'] == category]['filename']\n",
        "        for filename in category_files:\n",
        "            src = audio_dir / filename\n",
        "            dst = noise_dir / filename\n",
        "            if src.exists() and not dst.exists():\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "    print(f\"\\nNoise dataset prepared at: {noise_dir}\")\n",
        "    print(f\"Total noise samples: {len(list(noise_dir.glob('*.wav')))}\")\n",
        "\n",
        "    noise_files = df[df['category'].isin(noise_categories)]\n",
        "    category_stats = noise_files['category'].value_counts()\n",
        "    print(\"\\nSamples per category:\")\n",
        "    for category, count in category_stats.items():\n",
        "        print(f\"{category}: {count}\")\n",
        "\n",
        "    return str(noise_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Download and prepare the dataset\n",
        "    noise_dir = download_esc50()\n",
        "    print(f\"\\nDataset preparation completed. Use this path in your training script:\")\n",
        "    print(f\"noise_dir = '{noise_dir}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTNzXQu2bWDQ",
        "outputId": "a8873a0f-4bad-4c85-ca30-cf7f52c620a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ESC-50 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ESC-50: 646MB [00:45, 14.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Preparing noise samples...\n",
            "\n",
            "Noise dataset prepared at: /content/datasets/noise_samples\n",
            "Total noise samples: 520\n",
            "\n",
            "Samples per category:\n",
            "thunderstorm: 40\n",
            "chainsaw: 40\n",
            "airplane: 40\n",
            "train: 40\n",
            "wind: 40\n",
            "footsteps: 40\n",
            "crackling_fire: 40\n",
            "helicopter: 40\n",
            "rain: 40\n",
            "engine: 40\n",
            "sea_waves: 40\n",
            "siren: 40\n",
            "crickets: 40\n",
            "\n",
            "Dataset preparation completed. Use this path in your training script:\n",
            "noise_dir = '/content/datasets/noise_samples'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "# Constants\n",
        "SAMPLE_RATE = 44100\n",
        "MAX_DURATION = 5\n",
        "N_MELS = 64\n",
        "\n",
        "class CachedAudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, cache_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Dataset that loads pre-augmented and cached spectrograms\n",
        "        \"\"\"\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load metadata\n",
        "        with open(self.cache_dir / 'metadata.json', 'r') as f:\n",
        "            self.metadata = json.load(f)\n",
        "\n",
        "        self.files = list(self.metadata.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.files[idx]\n",
        "        # Load cached spectrogram\n",
        "        spec_path = self.cache_dir / f\"{filename}.pt\"\n",
        "        mel_spec = torch.load(spec_path)\n",
        "        label = self.metadata[filename]['label']\n",
        "\n",
        "        if self.transform:\n",
        "            mel_spec = self.transform(mel_spec)\n",
        "\n",
        "        return mel_spec, label\n",
        "\n",
        "def prepare_augmented_dataset(\n",
        "    root_dir,\n",
        "    noise_dir,\n",
        "    cache_dir,\n",
        "    num_augmentations=3,\n",
        "    min_snr_db=5,\n",
        "    max_snr_db=20,\n",
        "    force_rebuild=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepare and cache augmented dataset\n",
        "\n",
        "    Args:\n",
        "        root_dir: Directory containing original audio files\n",
        "        noise_dir: Directory containing noise samples\n",
        "        cache_dir: Directory to store augmented spectrograms\n",
        "        num_augmentations: Number of augmented versions per original file\n",
        "        min_snr_db: Minimum signal-to-noise ratio\n",
        "        max_snr_db: Maximum signal-to-noise ratio\n",
        "        force_rebuild: If True, rebuild cache even if it exists\n",
        "    \"\"\"\n",
        "    cache_dir = Path(cache_dir)\n",
        "\n",
        "    # Check if cache exists and is complete\n",
        "    if not force_rebuild and cache_dir.exists():\n",
        "        if (cache_dir / 'metadata.json').exists():\n",
        "            print(\"Found existing cached dataset\")\n",
        "            return cache_dir\n",
        "\n",
        "    # Create cache directory\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Initialize mel spectrogram transform\n",
        "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_mels=N_MELS,\n",
        "        normalized=True\n",
        "    )\n",
        "\n",
        "    # Load noise files\n",
        "    print(\"Loading noise files...\")\n",
        "    noise_files = list(Path(noise_dir).glob('*.wav'))\n",
        "\n",
        "    metadata = {}\n",
        "\n",
        "    # Process original files\n",
        "    print(\"Processing original files and creating augmentations...\")\n",
        "\n",
        "    # Process positive examples (yes_drone)\n",
        "    pos_dir = Path(root_dir) / 'yes_drone'\n",
        "    neg_dir = Path(root_dir) / 'unknown'\n",
        "\n",
        "    all_files = list(pos_dir.glob('*.wav')) + list(neg_dir.glob('*.wav'))\n",
        "    total_files = len(all_files) * (num_augmentations + 1)  # +1 for original\n",
        "\n",
        "    with tqdm(total=total_files, desc=\"Processing audio files\") as pbar:\n",
        "        for audio_path in all_files:\n",
        "            is_positive = audio_path.parent.name == 'yes_drone'\n",
        "\n",
        "            # Load and process original file\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            # Process original version\n",
        "            processed_waveform = process_waveform(waveform, sample_rate)\n",
        "            mel_spec = create_spectrogram(processed_waveform, mel_spectrogram)\n",
        "\n",
        "            # Save original\n",
        "            filename = f\"orig_{audio_path.stem}\"\n",
        "            torch.save(mel_spec, cache_dir / f\"{filename}.pt\")\n",
        "            metadata[filename] = {\n",
        "                'source': str(audio_path),\n",
        "                'label': 1 if is_positive else 0,\n",
        "                'augmented': False\n",
        "            }\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Create augmented versions\n",
        "            for aug_idx in range(num_augmentations):\n",
        "                # Select random noise file\n",
        "                noise_path = random.choice(noise_files)\n",
        "                noise_waveform, noise_sr = torchaudio.load(noise_path)\n",
        "\n",
        "                # Process noise\n",
        "                processed_noise = process_waveform(noise_waveform, noise_sr)\n",
        "\n",
        "                # Apply noise\n",
        "                snr_db = random.uniform(min_snr_db, max_snr_db)\n",
        "                noisy_waveform = add_noise(processed_waveform, processed_noise, snr_db)\n",
        "\n",
        "                # Create and save spectrogram\n",
        "                mel_spec = create_spectrogram(noisy_waveform, mel_spectrogram)\n",
        "                filename = f\"aug_{audio_path.stem}_{aug_idx}\"\n",
        "                torch.save(mel_spec, cache_dir / f\"{filename}.pt\")\n",
        "\n",
        "                metadata[filename] = {\n",
        "                    'source': str(audio_path),\n",
        "                    'label': 1 if is_positive else 0,\n",
        "                    'augmented': True,\n",
        "                    'noise_source': str(noise_path),\n",
        "                    'snr_db': snr_db\n",
        "                }\n",
        "                pbar.update(1)\n",
        "\n",
        "    # Save metadata\n",
        "    with open(cache_dir / 'metadata.json', 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\nDataset cached at: {cache_dir}\")\n",
        "    print(f\"Total files: {len(metadata)}\")\n",
        "    return cache_dir\n",
        "\n",
        "def process_waveform(waveform, sample_rate):\n",
        "    \"\"\"Process waveform to standard format\"\"\"\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Resample if necessary\n",
        "    if sample_rate != SAMPLE_RATE:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Pad or trim to MAX_DURATION\n",
        "    target_length = MAX_DURATION * SAMPLE_RATE\n",
        "    if waveform.shape[1] < target_length:\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, target_length - waveform.shape[1]))\n",
        "    else:\n",
        "        waveform = waveform[:, :target_length]\n",
        "\n",
        "    return waveform\n",
        "\n",
        "def create_spectrogram(waveform, mel_spectrogram):\n",
        "    \"\"\"Create mel spectrogram from waveform\"\"\"\n",
        "    mel_spec = mel_spectrogram(waveform)\n",
        "    mel_spec = torch.log(mel_spec + 1e-9)\n",
        "    return mel_spec\n",
        "\n",
        "def add_noise(signal, noise, target_snr_db):\n",
        "    \"\"\"Add noise to signal at specified SNR\"\"\"\n",
        "    signal_power = torch.mean(signal ** 2)\n",
        "    noise_power = torch.mean(noise ** 2)\n",
        "\n",
        "    snr = 10 ** (target_snr_db / 10)\n",
        "    scale = torch.sqrt(signal_power / (noise_power * snr))\n",
        "\n",
        "    noisy_signal = signal + scale * noise\n",
        "\n",
        "    # Normalize to prevent clipping\n",
        "    max_val = torch.max(torch.abs(noisy_signal))\n",
        "    if max_val > 1:\n",
        "        noisy_signal = noisy_signal / max_val\n",
        "\n",
        "    return noisy_signal\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cache_dir = prepare_augmented_dataset(\n",
        "        root_dir='DroneAudioDataset/Binary_Drone_Audio',\n",
        "        noise_dir='/content/datasets/noise_samples',\n",
        "        cache_dir='augmented_data',\n",
        "        num_augmentations=3,  # Number of augmented versions per original file\n",
        "        force_rebuild=False\n",
        "    )\n",
        "\n",
        "    # Create dataset from cache\n",
        "    dataset = CachedAudioDataset(cache_dir)\n",
        "    print(f\"Dataset size: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CvidpW4H9T_",
        "outputId": "bf6eaa59-77c3-4f40-dd4f-fa0ed4a2fc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading noise files...\n",
            "Processing original files and creating augmentations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing audio files: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset cached at: augmented_data\n",
            "Total files: 0\n",
            "Dataset size: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "# Constants\n",
        "SAMPLE_RATE = 44100\n",
        "MAX_DURATION = 5\n",
        "N_MELS = 64\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class NoiseAugment:\n",
        "    def __init__(self, noise_dir, min_snr_db=5, max_snr_db=20):\n",
        "        self.noise_dir = Path(noise_dir)\n",
        "        self.min_snr_db = min_snr_db\n",
        "        self.max_snr_db = max_snr_db\n",
        "        self.noise_files = list(self.noise_dir.glob('*.wav'))\n",
        "\n",
        "        if not self.noise_files:\n",
        "            raise ValueError(f\"No .wav files found in {noise_dir}\")\n",
        "\n",
        "    def load_random_noise(self, target_length):\n",
        "        noise_path = random.choice(self.noise_files)\n",
        "        noise, sample_rate = torchaudio.load(noise_path)\n",
        "\n",
        "        if noise.shape[0] > 1:\n",
        "            noise = torch.mean(noise, dim=0, keepdim=True)\n",
        "\n",
        "        if sample_rate != SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE)\n",
        "            noise = resampler(noise)\n",
        "\n",
        "        if noise.shape[1] < target_length:\n",
        "            num_repeats = (target_length + noise.shape[1] - 1) // noise.shape[1]\n",
        "            noise = noise.repeat(1, num_repeats)\n",
        "\n",
        "        start = random.randint(0, noise.shape[1] - target_length)\n",
        "        noise = noise[:, start:start + target_length]\n",
        "\n",
        "        return noise\n",
        "\n",
        "    def apply_noise(self, waveform):\n",
        "        noise = self.load_random_noise(waveform.shape[1])\n",
        "\n",
        "        signal_power = torch.mean(waveform ** 2)\n",
        "        noise_power = torch.mean(noise ** 2)\n",
        "\n",
        "        target_snr_db = random.uniform(self.min_snr_db, self.max_snr_db)\n",
        "        snr = 10 ** (target_snr_db / 10)\n",
        "        scale = torch.sqrt(signal_power / (noise_power * snr))\n",
        "\n",
        "        noisy_waveform = waveform + scale * noise\n",
        "\n",
        "        max_val = torch.max(torch.abs(noisy_waveform))\n",
        "        if max_val > 1:\n",
        "            noisy_waveform = noisy_waveform / max_val\n",
        "\n",
        "        return noisy_waveform\n",
        "\n",
        "class AugmentedAudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, noise_dir, transform=None, aug_probability=0.5):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.aug_probability = aug_probability\n",
        "        self.noise_augmenter = NoiseAugment(noise_dir)\n",
        "\n",
        "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_mels=N_MELS,\n",
        "            normalized=True\n",
        "        )\n",
        "\n",
        "        # Get all file paths and labels\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Positive examples (yes_drone)\n",
        "        pos_dir = self.root_dir / 'yes_drone'\n",
        "        for file in pos_dir.glob('*.wav'):\n",
        "            self.files.append(file)\n",
        "            self.labels.append(1)\n",
        "\n",
        "        # Negative examples (unknown)\n",
        "        neg_dir = self.root_dir / 'unknown'\n",
        "        for file in neg_dir.glob('*.wav'):\n",
        "            self.files.append(file)\n",
        "            self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        if sample_rate != SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(sample_rate, SAMPLE_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        target_length = MAX_DURATION * SAMPLE_RATE\n",
        "        if waveform.shape[1] < target_length:\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, target_length - waveform.shape[1]))\n",
        "        else:\n",
        "            waveform = waveform[:, :target_length]\n",
        "\n",
        "        if random.random() < self.aug_probability:\n",
        "            waveform = self.noise_augmenter.apply_noise(waveform)\n",
        "\n",
        "        mel_spec = self.mel_spectrogram(waveform)\n",
        "        mel_spec = torch.log(mel_spec + 1e-9)\n",
        "\n",
        "        if self.transform:\n",
        "            mel_spec = self.transform(mel_spec)\n",
        "\n",
        "        return mel_spec, label\n",
        "\n",
        "class DroneClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DroneClassifier, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 4 * 4, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "        print('-' * 60)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "            }, f'best_drone_classifier_{timestamp}.pth')\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Acc')\n",
        "    plt.plot(val_accs, label='Val Acc')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_history_{timestamp}.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(234)\n",
        "    random.seed(542)\n",
        "    np.random.seed(7463)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = AugmentedAudioDataset(\n",
        "        root_dir='DroneAudioDataset/Binary_Drone_Audio',\n",
        "        noise_dir='/content/datasets/noise_samples',\n",
        "        aug_probability=0.5\n",
        "    )\n",
        "\n",
        "    # Split dataset\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(dataset)),\n",
        "        test_size=0.2,\n",
        "        random_state=52,\n",
        "        stratify=dataset.labels\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "    )\n",
        "\n",
        "    # Initialize model, criterion, and optimizer\n",
        "    model = DroneClassifier().to(DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # Train model\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, DEVICE)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "FxPe2ZNjW31v",
        "outputId": "6bf70d46-1e31-4eae-b8f0-3efb2f4e9bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30]\n",
            "Train Loss: 0.2475, Train Acc: 88.61%\n",
            "Val Loss: 0.1595, Val Acc: 88.64%\n",
            "------------------------------------------------------------\n",
            "Epoch [2/30]\n",
            "Train Loss: 0.1633, Train Acc: 88.61%\n",
            "Val Loss: 0.1801, Val Acc: 88.64%\n",
            "------------------------------------------------------------\n",
            "Epoch [3/30]\n",
            "Train Loss: 0.1453, Train Acc: 88.61%\n",
            "Val Loss: 0.1689, Val Acc: 88.64%\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7d57e7791fc2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-7d57e7791fc2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7d57e7791fc2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtrain_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7d57e7791fc2>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}